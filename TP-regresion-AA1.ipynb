{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, Ridge, ElasticNet, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import holidays\n",
    "\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el dataset en un dataframe\n",
    "df = pd.read_csv('uber_fares.csv')\n",
    "\n",
    "# Revisa si hay filas duplicadas\n",
    "df.duplicated().sum() # 0 filas duplicadas\n",
    "\n",
    "# Revisa las columnas y sus tipos de datos\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asigna el tipo de datos correcto a las variables que representan fechas\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime']).dt.floor('s')\n",
    "df['date'] = pd.to_datetime(df['date']).dt.floor('s')\n",
    "\n",
    "# Muestra las primeras filas del dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Limpieza y preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina la columna 'key' que no aporta información relevante\n",
    "df = df.drop(columns=['key'])\n",
    "\n",
    "# Chequea si las columnas 'pickup_datetime' y 'date' son iguales\n",
    "df['pickup_datetime'].equals(df['date']) # True\n",
    "\n",
    "# Elimina la columna 'pickup_datetime' ya que es redundante\n",
    "df = df.drop(columns=['pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina las filas cuya variable objetivo no es un valor posible\n",
    "df = df[df['fare_amount'] > 0] # 0.01% de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Null Island\n",
    "Isla ficticia, ubicada en 0°N 0°E, que los GPS suelen utilizar como ubicación por defecto cuando fallan y no pueden determinar la ubicación real, es decir, representa una ubicación nula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sin_null_island = df[\n",
    "    ~(\n",
    "        (df['pickup_latitude'] == 0) & (df['pickup_longitude'] == 0) |\n",
    "        (df['dropoff_latitude'] == 0) & (df['dropoff_longitude'] == 0)\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Los viajes que comenzaron o terminaron en Null Island representan el {100 * (1 - df_sin_null_island.shape[0] / df.shape[0]):.2f}% de los datos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina las filas que corresponden a viajes que empezaron o terminaron en Null Island\n",
    "df = df_sin_null_island"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Visualización de las variables de coordenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera un nuevo dataframe con todas las coordenadas\n",
    "coordenadas = pd.concat(\n",
    "    [\n",
    "        df[['pickup_latitude', 'pickup_longitude']].rename(\n",
    "            columns={'pickup_latitude': 'latitude', 'pickup_longitude': 'longitude'}\n",
    "        ).assign(type='pickup'),\n",
    "        df[['dropoff_latitude', 'dropoff_longitude']].rename(\n",
    "            columns={'dropoff_latitude': 'latitude', 'dropoff_longitude': 'longitude'}\n",
    "        ).assign(type='dropoff')\n",
    "    ], ignore_index=True\n",
    ")\n",
    "\n",
    "# Muestra un mapa de las ubicaciones de inicio y fin del viaje\n",
    "fig = px.scatter_map(\n",
    "    coordenadas.sample(50000),\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    color=\"type\",\n",
    "    zoom=5,\n",
    "    title=\"Ubicaciones\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0}  # elimina márgenes blancos\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "A simple vista se observa que hay una gran densidad de viajes en Nueva York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "limites_ny = {\n",
    "    'lat_min': 40.49,\n",
    "    'lat_max': 40.92,\n",
    "    'lon_min': -74.27,\n",
    "    'lon_max': -73.68\n",
    "}\n",
    "\n",
    "cantidad_viajes = df.shape[0]\n",
    "\n",
    "# Filtra los viajes que se dieron de los límites de Nueva York\n",
    "df_ny = df[\n",
    "    (df['pickup_latitude'] >= limites_ny['lat_min']) &\n",
    "    (df['pickup_latitude'] <= limites_ny['lat_max']) &\n",
    "    (df['pickup_longitude'] >= limites_ny['lon_min']) &\n",
    "    (df['pickup_longitude'] <= limites_ny['lon_max']) &\n",
    "    (df['dropoff_latitude'] >= limites_ny['lat_min']) &\n",
    "    (df['dropoff_latitude'] <= limites_ny['lat_max']) &\n",
    "    (df['dropoff_longitude'] >= limites_ny['lon_min']) &\n",
    "    (df['dropoff_longitude'] <= limites_ny['lon_max'])\n",
    "]\n",
    "cantidad_viajes_ny = df_ny.shape[0]\n",
    "\n",
    "print(f\"Los viajes dentro de los límites de Nueva York representan el {cantidad_viajes_ny / cantidad_viajes * 100:.2f}% del total.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "TODO Vamos a reducir el alcance de nuestro modelo predictivo a viajes integramente dentro de la ciudad de Nueva York, ya que etc COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina las filas que corresponden a viajes que no se dieron dentro de los límites de Nueva York\n",
    "df = df_ny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Los valores posibles para la variable *passenger_count* están en el rango [0;6]. (UberX permite hasta 4 y UberXL hasta 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de viajes con cantidad de pasajeros no válida\n",
    "df[(df['passenger_count'] > 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina la fila con passenger_count = 208, que es la única con valor absurdo.\n",
    "df = df[df['passenger_count'] <= 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Hasta este momento eliminamos un {100 - df.shape[0] / 200000 * 100:.2f}% de los datos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Generación de variable distancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputar_distancia(viaje):\n",
    "    '''\n",
    "    Calcula la distancía  en kilometros del viaje\n",
    "    mediante una combinación de distancia Manhattan con\n",
    "    distancia geodésica (teniendo en cuenta la curvatura\n",
    "    de la Tierra.)\n",
    "    '''\n",
    "    lat1 = viaje['pickup_latitude']\n",
    "    lon1 = viaje['pickup_longitude']\n",
    "    lat2 = viaje['dropoff_latitude']\n",
    "    lon2 = viaje['dropoff_longitude']\n",
    "\n",
    "    distancia_lat = np.float32(geodesic((lat1, lon1), (lat2, lon1)).kilometers)\n",
    "    distancia_lon = np.float32(geodesic((lat2, lon1), (lat2, lon2)).kilometers)\n",
    "\n",
    "    return distancia_lat + distancia_lon\n",
    "\n",
    "df['distance'] = df.apply(imputar_distancia, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa el 80% para train y 20% para test\n",
    "train, test= train_test_split(df, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de variables\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(data=train, x='fare_amount', bins=50, color=sns.color_palette(\"muted\")[0], ax=axes[0], edgecolor='none')\n",
    "sns.histplot(data=train, x='distance', bins=50, color=sns.color_palette(\"muted\")[1], ax=axes[1], edgecolor='none')\n",
    "sns.countplot(data=train, x='passenger_count', color=sns.color_palette(\"muted\")[2], ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Se observa que tanto *fare_amount* como *distance* están sesgadas hacía la derecha. Vamos a tratar sus outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Tratado de Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Variable *fare_amount*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['fare_amount'].describe(percentiles=[0.001, 0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99, 0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de tarifas mayores al 99.9% de los datos.\n",
    "sns.histplot(data=train[train['fare_amount'] > 75], x='fare_amount', bins=50, color=sns.color_palette(\"muted\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Se obversan 4 valores atípicos de tarifa que se separan mucho del resto de los datos y también entre si. Los reemplazamos por valores nulos para su posterior imputación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['fare_amount'] = np.where(train['fare_amount'] > 150, np.nan, train['fare_amount'])\n",
    "test['fare_amount'] = np.where(test['fare_amount'] > 150, np.nan, test['fare_amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de las tarifas hasta el percentil 1%\n",
    "train[train['fare_amount'] <= 3.3]['fare_amount'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Se observan 2 valores de tarifa muy inferiores en valor y frecuencia a los demás datos. Las reemplazamos por nulos para para su posterior imputación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['fare_amount'] = np.where(train['fare_amount'] < 1, np.nan, train['fare_amount'])\n",
    "test['fare_amount'] = np.where(test['fare_amount'] < 1, np.nan, test['fare_amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "#### Variable *distance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['distance'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "La distancia del viaje no puede ser 0, porque no habría viaje. Asumimos que corresponden a viajes que comienzan y terminan en la misma ubicación pero tienen un recorrido con paradas intermedias, como no tenemos información sobre paradas intermedias, reemplazamos los valores 0 de distancia por nulos para su posterior imputacíon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.assign(distance=lambda x: x['distance'].replace(0, np.nan))\n",
    "test = test.assign(distance=lambda x: x['distance'].replace(0, np.nan))\n",
    "\n",
    "# Estadísticas descriptivas de distancias no nulas\n",
    "train['distance'].describe(percentiles=[0.001, 0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99, 0.999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "No vamos a considerar como válida ninguna distancia menor a 0.1km. Reemplazamos por null para posterior imputación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['distance'] = np.where(train['distance'] < 0.1, np.nan, train['distance'])\n",
    "test['distance'] = np.where(test['distance'] < 0.1, np.nan, test['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarta viajes sin tarifa y sin distancia. No imputables.\n",
    "train = train[~(train['fare_amount'].isnull() & train['distance'].isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Ingenieria de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "#### Variable *date*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hour'] = train['date'].dt.hour\n",
    "train['day'] = train['date'].dt.day_of_week\n",
    "train['month'] = train['date'].dt.month\n",
    "\n",
    "test['hour'] = test['date'].dt.hour\n",
    "test['day'] = test['date'].dt.day_of_week\n",
    "test['month'] = test['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "\n",
    "dias_semana = [\"Lunes\", \"Martes\", \"Miércoles\", \"Jueves\", \"Viernes\", \"Sábado\", \"Domingo\"]\n",
    "\n",
    "for i, dia in enumerate(dias_semana):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    sns.histplot(\n",
    "        data=train[train[\"day\"] == i],\n",
    "        x=\"hour\",\n",
    "        bins=24,\n",
    "        color=sns.color_palette(\"muted\", 7)[i],\n",
    "        edgecolor=\"none\",\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(dia)\n",
    "    ax.set_xlabel(\"Horas\")\n",
    "    ax.set_ylabel(\"Frecuencia\")\n",
    "    ax.set_xticks(range(0, 24, 2))\n",
    "\n",
    "fig.delaxes(axes[2, 1])\n",
    "fig.delaxes(axes[2, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Se observa que los días Lunes, Martes y Miércoles presentan una distribución muy parecida, con una rápida diminución de viajes luego de las 22 horas.\n",
    "\n",
    "\n",
    "Los Jueves, Viernes y Sábados presentan una mayor cantidad de actividad en horarios nocturnos, y un aumento en la demanda luego de las 22 horas, especialmente los viernes y sábados.\n",
    "\n",
    "\n",
    "Los domingos presentan un comportamiento único, teníendo su momento de mayor actividad a la madrugada y una baja de actividad a partir de las 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera variables dummys de grupos de dias de la semana con distribuciones similares\n",
    "train['is_mon_tue_wed'] = np.where(train['day'].isin([0, 1, 2]), 1, 0)\n",
    "train['is_thu_fri_sat'] = np.where(train['day'].isin([3, 4, 5]), 1, 0)\n",
    "\n",
    "test['is_mon_tue_wed'] = np.where(test['day'].isin([0, 1, 2]), 1, 0)\n",
    "test['is_thu_fri_sat'] = np.where(test['day'].isin([3, 4, 5]), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarifa_promedio_por_hora = train.groupby(\"hour\")['fare_amount'].mean().reset_index(name=\"promedio\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=tarifa_promedio_por_hora,\n",
    "    x=\"hour\",\n",
    "    y=\"promedio\",\n",
    "    hue=\"hour\",\n",
    "    palette=\"muted\",\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "plt.title(\"Tarifa promedio por hora del día\")\n",
    "plt.xlabel(\"Hora\")\n",
    "plt.ylabel(\"Tarifa Promedio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Las tárifas promedio mas altas se dan a las 4 y 5 de la madrugada de forma acentuada por sobre el resto de las horas del día."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera variable dummy para indicar horario de madrugada\n",
    "train['is_early_morning'] = np.where(train['hour'].isin([4, 5]), 1, 0)\n",
    "\n",
    "test['is_early_morning'] = np.where(test['hour'].isin([4, 5]), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "viajes_por_hora = train.groupby(\"hour\")['fare_amount'].size().reset_index(name=\"frecuencia\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=viajes_por_hora,\n",
    "    x=\"hour\",\n",
    "    y=\"frecuencia\",\n",
    "    hue=\"hour\",\n",
    "    palette=\"muted\",\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "plt.title(\"Cantidad de viajes por hora del día\")\n",
    "plt.xlabel(\"Hora\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hour'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "El horario de mayor actividad general se da marcadamente entre las 18 y las 22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera variable dummy de alta actividad\n",
    "train['is_high_activity'] = np.where(train['hour'].isin([18, 19, 20, 21, 22]), 1, 0)\n",
    "\n",
    "test['is_high_activity'] = np.where(test['hour'].isin([18, 19, 20, 21, 22]), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarifa_promedio_por_mes = train.groupby(\"month\")['fare_amount'].mean().reset_index(name=\"promedio\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=tarifa_promedio_por_mes,\n",
    "    x=\"month\",\n",
    "    y=\"promedio\",\n",
    "    hue=\"month\",\n",
    "    palette=\"muted\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "plt.xticks(\n",
    "    ticks=range(12),\n",
    "    labels=[\"Enero\", \"Febrero\", \"Marzo\", \"Abril\", \"Mayo\", \"Junio\", \"Julio\", \"Agosto\", \"Septiembre\", \"Octubre\", \"Noviembre\", \"Diciembre\"],\n",
    "    rotation=45\n",
    ")\n",
    "\n",
    "plt.title(\"Tarifa promedio por mes del año\")\n",
    "plt.xlabel(\"Mes\")\n",
    "plt.ylabel(\"Tarifa Promedio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "La tarifa promedio a lo largo de los meses no muestra ninguna tendencia o particularidad clara por lo que decidimos no incluir ninguna variable relacionada para reducir el riesgo de overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea variable dummy para identificar viajes en días feriados\n",
    "us_holidays = holidays.US(years=train[\"date\"].dt.year.unique(), state=\"NY\")\n",
    "train[\"is_holiday\"] = train[\"date\"].dt.date.isin(us_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(\"is_holiday\")['fare_amount'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "El hecho de que sea o no feriado no parece afectar el valor de la tarifa. por lo que decidimos eliminarla para reducir el riesgo de overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['is_holiday'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "#### Variable *passenger_count*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### Escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train[['distance', 'fare_amount']])\n",
    "\n",
    "train[['distance', 'fare_amount']] = scaler.transform(train[['distance', 'fare_amount']])\n",
    "test[['distance', 'fare_amount']] = scaler.transform(test[['distance', 'fare_amount']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Imputación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputa los valores faltantes de las variables numéricas fare_amount y distance mediante KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputer.fit(train[['distance', 'fare_amount']])\n",
    "\n",
    "train[['distance', 'fare_amount']] = imputer.transform(train[['distance', 'fare_amount']])\n",
    "test[['distance', 'fare_amount']] = imputer.transform(test[['distance', 'fare_amount']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train,\n",
    "    x='distance',\n",
    "    y='fare_amount',\n",
    "    color=sns.color_palette(\"muted\")[0],\n",
    ")\n",
    "\n",
    "plt.xlim(-1, 5)\n",
    "plt.ylim(-1, 5)\n",
    "plt.xlabel(\"Distancia (escalada)\")\n",
    "plt.ylabel(\"Tarifa (escalada)\")\n",
    "plt.title(\"Relación entre distancia y tarifa (escaladas)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarifa_promedio_por_cantidad_pasajeros = train.groupby(\"passenger_count\")['fare_amount'].mean().reset_index(name=\"promedio\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=tarifa_promedio_por_cantidad_pasajeros,\n",
    "    x=\"passenger_count\",\n",
    "    y=\"promedio\",\n",
    "    hue=\"passenger_count\",\n",
    "    palette=\"muted\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "plt.title(\"Tarifa promedio por cantidad de pasajeros\")\n",
    "plt.xlabel(\"Cantidad de pasajeros\")\n",
    "plt.ylabel(\"Tarifa Promedio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "Si bien la cantidad exacta de pasajeros de un viaje es un dato desconocido antes de concretar el viaje, si sabemos que los viajes con 0 pasajeros son envíos mediante \"Uber Flash\", y los viajes de 5 o 6 pasajeros necesitan vehículos aprobados y se solicitan bajo el nombre \"Uber XL\". Est información si la tenemos disponible al momento de solicitar el viaje ya que forma parte de los parametros que configuran al mismo. \n",
    "\n",
    "\n",
    "A priori se observa que el promedio de tarifa de cadeteria mediante \"Uber Flash\" es inferior a cualquier viaje con pasajeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['type_of_service'] = np.where(train['passenger_count'] == 0, 'flash',\n",
    "                                    np.where(train['passenger_count'] > 5, 'xl', 'x'))\n",
    "\n",
    "test['type_of_service'] = np.where(test['passenger_count'] == 0, 'flash',\n",
    "                                    np.where(test['passenger_count'] > 5, 'xl', 'x'))\n",
    "\n",
    "tarifa_promedio_por_cantidad_pasajeros = train.groupby(\"type_of_service\")['fare_amount'].mean().reset_index(name=\"promedio\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=tarifa_promedio_por_cantidad_pasajeros,\n",
    "    x=\"type_of_service\",\n",
    "    y=\"promedio\",\n",
    "    hue=\"type_of_service\",\n",
    "    palette=\"muted\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "plt.title(\"Tarifa promedio por cantidad de pasajeros\")\n",
    "plt.xlabel(\"Cantidad de pasajeros\")\n",
    "plt.ylabel(\"Tarifa Promedio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=train[train['fare_amount'].quantile(0.99) >= train['fare_amount']], # Omite outliers para mejor visualización\n",
    "    x='fare_amount',\n",
    "    hue='type_of_service',\n",
    "    palette='muted',\n",
    ")\n",
    "\n",
    "plt.title('Distribución de valor de tarifa por tipo de servicio')\n",
    "plt.xlabel('Tarifa (USD)')\n",
    "plt.legend(title='Tipo de servicio')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "Se observan diferencias claras entre las tarifas de los distintos servicios, consistentemente las tarifas de servicio \"Uber Flash\" son inferiores a las de \"Uber X\", que son inferiores a las de \"Uber XL\". Vamos a considerar esta información para el modelo mediante el uso de variables dummys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera dummys para tipo de servicio\n",
    "train['is_xl'] = np.where(train['type_of_service'] == 'xl', 1, 0)\n",
    "train['is_flash'] = np.where(train['type_of_service'] == 'flash', 1, 0)\n",
    "\n",
    "test['is_xl'] = np.where(test['type_of_service'] == 'xl', 1, 0)\n",
    "test['is_flash'] = np.where(test['type_of_service'] == 'flash', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "## PreTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina columnas que no se van a utilizar en el modelo\n",
    "train.drop(columns=['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'type_of_service', 'date', 'hour', 'day', 'month'], inplace=True)\n",
    "test.drop(columns=['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'type_of_service', 'date', 'hour', 'day', 'month'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "# Entrenamiento de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(columns=['fare_amount'])\n",
    "y_train = train['fare_amount']\n",
    "\n",
    "x_test = test.drop(columns=['fare_amount'])\n",
    "y_test = test['fare_amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "## Regresión Lineal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train = cross_val_score(lr, x_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_train, y_train)\n",
    "\n",
    "y_test_pred = lr.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_test - y_test_pred))\n",
    "\n",
    "print(f\"Train R²: {r2_train:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeficientes = pd.DataFrame({\n",
    "    'Variable': x_train.columns,\n",
    "    'Coeficiente': lr.coef_\n",
    "}).sort_values(by='Coeficiente', ascending=False)\n",
    "\n",
    "coeficientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# desescalo las predicciones y los valores reales de tarifa (variable target)\n",
    "y_test_original = scaler.inverse_transform(np.column_stack((np.zeros_like(y_test), y_test)))[:, 1]\n",
    "y_pred_original = scaler.inverse_transform(np.column_stack((np.zeros_like(y_test_pred), y_test_pred)))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar diferencias entre valores reales y predichos\n",
    "plt.figure(figsize=(12, 6))\n",
    "# plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "\n",
    "sns.scatterplot(x=y_test_original, y=y_pred_original)\n",
    "plt.plot([y_test_original.min(), y_test_original.max()], [y_test_original.min(), y_test_original.max()],color='r', alpha=0.5, lw=2)\n",
    "\n",
    "plt.xlabel('Valores Reales')\n",
    "plt.ylabel('Valores Predichos')\n",
    "plt.title('Valores Reales vs Valores Predichos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuos = y_test_original - y_pred_original\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x=y_pred_original, y=residuos)\n",
    "plt.axhline(0, color='r', linestyle='solid', alpha=0.5, lw=2)\n",
    "plt.xlabel('Valores Predichos')\n",
    "plt.ylabel('Residuos')\n",
    "plt.title('Gráfico de Residuos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "### Descenso por gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, X_val, y_val, lr=0.01, epochs=100):\n",
    "    \"\"\"\n",
    "    shapes:\n",
    "        X_train = nxm\n",
    "        y_train = nx1\n",
    "        X_val = pxm\n",
    "        y_test = px1\n",
    "        W = mx1\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    \n",
    "    o = X_val.shape[0]\n",
    "\n",
    "    # Poner columna de unos a las matrices X\n",
    "    X_train = np.hstack((np.ones((n, 1)), X_train))\n",
    "    X_val = np.hstack((np.ones((o, 1)), X_val))\n",
    "    \n",
    "\n",
    "    # Inicializar pesos aleatorios\n",
    "    W = np.random.randn(m+1).reshape(m+1, 1)\n",
    "\n",
    "    train_errors = []  # Para almacenar el error de entrenamiento en cada época\n",
    "    test_errors = []   # Para almacenar el error de prueba en cada época\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Calcular predicción y error de entrenamiento\n",
    "        prediction_train = np.matmul(X_train, W) \n",
    "        error_train = y_train - prediction_train  \n",
    "        #print(error_train)\n",
    "        train_mse = np.mean(error_train ** 2)\n",
    "        train_errors.append(train_mse)\n",
    "\n",
    "        # Calcular predicción y error de prueba\n",
    "        prediction_test = np.matmul(X_val, W) \n",
    "        error_test = y_val - prediction_test \n",
    "        test_mse = np.mean(error_test ** 2)\n",
    "        test_errors.append(test_mse)\n",
    "\n",
    "        # Calcular el gradiente y actualizar pesos\n",
    "        grad_sum = np.sum(error_train * X_train, axis=0)\n",
    "        grad_mul = -2/n * grad_sum  # 1xm\n",
    "        gradient = np.transpose(grad_mul).reshape(-1, 1)  # mx1\n",
    "\n",
    "        W = W - (lr * gradient)\n",
    "\n",
    "    # Graficar errores de entrenamiento y prueba\n",
    "    # Definir una figura\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Plotear errores de entrenamiento\n",
    "    plt.plot(train_errors, label='Error de entrenamiento')\n",
    "    # Plotear errores de prueba\n",
    "    plt.plot(test_errors, label='Error de validación')\n",
    "    # Poner labels en los ejes\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Error cuadrático medio')\n",
    "    # Activar la leyenda\n",
    "    plt.legend()\n",
    "    # Poner titulo\n",
    "    plt.title('Error de entrenamiento y validación vs iteraciones (GD)')\n",
    "    # Terminar y mostrar gráfico\n",
    "    plt.show()\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds_manual(gradient_descent, x_train, y_train):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_train = k_folds_manual(gradient_descent, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = gradient_descent(x_train.values, y_train.values.reshape(-1, 1), x_test.values, y_test.values.reshape(-1, 1), lr=0.01, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.matmul(np.hstack((np.ones((x_test.shape[0], 1)), x_test.values)), gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pasar y_pred a 1D\n",
    "y_pred = y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadisticas de desempeño\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "\n",
    "# Imprime las estadísticas de desempeño\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "### Estocástico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(max_iter=100, tol=1e-3, learning_rate='constant', eta0=0.01, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train = cross_val_score(sgd, x_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.fit(x_train, y_train)\n",
    "\n",
    "y_test_pred = sgd.predict(x_test)\n",
    "\n",
    "# Estadisticas de desempeño\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_test - y_test_pred))\n",
    "\n",
    "# Imprime las estadísticas de desempeño\n",
    "print(f\"Train R²: {r2_train:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# desescalo las predicciones y los valores reales de tarifa (variable target)\n",
    "y_test_original = scaler.inverse_transform(np.column_stack((np.zeros_like(y_test), y_test)))[:, 1]\n",
    "y_pred_original = scaler.inverse_transform(np.column_stack((np.zeros_like(y_test_pred), y_test_pred)))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar diferencias entre valores reales y predichos\n",
    "plt.figure(figsize=(12, 6))\n",
    "# plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "\n",
    "sns.scatterplot(x=y_test_original, y=y_pred_original, alpha=1)\n",
    "plt.plot([y_test_original.min(), y_test_original.max()], [y_test_original.min(), y_test_original.max()], 'r--', lw=2)\n",
    "\n",
    "plt.xlabel('Valores Reales')\n",
    "plt.ylabel('Valores Predichos')\n",
    "plt.title('Valores Reales vs Valores Predichos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuos =  y_pred_original - y_test_original\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x=y_test_original, y=residuos, alpha=1)\n",
    "plt.axhline(0, color='r', linestyle='solid', lw=2, alpha=0.5)\n",
    "plt.xlabel('Tarifas reales')\n",
    "plt.ylabel('Residuos')\n",
    "plt.title('Gráfico de Residuos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "### MiniBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X_train, y_train, X_test, y_test, lr=0.01, epochs=100, batch_size=11):\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "\n",
    "    X_train = np.hstack((np.ones((n, 1)), X_train))\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "\n",
    "    W = np.random.randn(m + 1).reshape(-1, 1)\n",
    "\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Permutación aleatoria de los datos\n",
    "        permutation = np.random.permutation(n)\n",
    "        X_train = X_train[permutation]\n",
    "        y_train = y_train[permutation]\n",
    "\n",
    "\n",
    "        for j in range(0, n, batch_size):\n",
    "            # Obtener un lote (mini-batch) de datos\n",
    "            x_batch = X_train[j:j+batch_size, :]\n",
    "            y_batch = y_train[j:j+batch_size].reshape(-1, 1)\n",
    "\n",
    "            prediction = np.matmul(x_batch, W)\n",
    "            error = y_batch - prediction\n",
    "            train_mse = np.mean(error ** 2)\n",
    "            train_errors.append(train_mse)\n",
    "\n",
    "            gradient = -2 * np.matmul(x_batch.T, error) / batch_size\n",
    "\n",
    "            W = W - (lr * gradient)\n",
    "\n",
    "            prediction_test = np.matmul(X_test, W)\n",
    "            error_test = y_test - prediction_test\n",
    "            test_mse = np.mean(error_test ** 2)\n",
    "            test_errors.append(test_mse)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_errors, label='Error de entrenamiento')\n",
    "    plt.plot(test_errors, label='Error de prueba')\n",
    "    plt.xlabel('Iteración')\n",
    "    plt.ylabel('Error cuadrático medio')\n",
    "    plt.legend()\n",
    "    plt.title('Error de entrenamiento y prueba vs iteraciones (Mini-Batch GD)')\n",
    "    plt.show()\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbgd = mini_batch_gradient_descent(x_train.values, y_train.values.reshape(-1, 1), x_test.values, y_test.values.reshape(-1, 1), lr=0.01, epochs=100, batch_size=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.matmul(np.hstack((np.ones((x_test.shape[0], 1)), x_test)), mbgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pasar y_pred a 1D\n",
    "y_pred = y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadisticas de desempeño\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "\n",
    "# Imprime las estadísticas de desempeño\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeCV(\n",
    "    alphas          = np.logspace(-3, 8, 200),\n",
    "    fit_intercept   = True,\n",
    "    store_cv_results = True\n",
    ")\n",
    "\n",
    "ridge.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = ridge.alphas\n",
    "coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge_aux = Ridge(alpha=alpha)\n",
    "    ridge_aux.fit(x_train, y_train)\n",
    "    coefs.append(ridge_aux.coef_.flatten())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "ax.plot(alphas, coefs, label=x_train.columns)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('log(alpha)')\n",
    "ax.set_ylabel('Parámetros')\n",
    "ax.set_title('Parámetros del modelo en función de la regularización')\n",
    "plt.axis('tight')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_cv = ridge.cv_results_.reshape((-1, 200)).mean(axis=0)\n",
    "\n",
    "# Se aplica la raíz cuadrada para pasar de mse a rmse\n",
    "rmse_cv = np.sqrt(mse_cv)\n",
    "\n",
    "# Se identifica el mejor\n",
    "min_rmse     = np.min(rmse_cv)\n",
    "optimo       = ridge.alphas[np.argmin(rmse_cv)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(ridge.alphas, rmse_cv)\n",
    "ax.set_xscale('log')\n",
    "ax.set_title('Evolución del error en función de la regularización')\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('RMSE')\n",
    "\n",
    "ax.plot(optimo, min_rmse, marker='o', markersize=10, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métrica de train cross-validation\n",
    "print(f\"El valor óptimo de alpha es: {optimo:.2f} con un RMSE de {min_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficientes del modelo\n",
    "\n",
    "df_coeficientes = pd.DataFrame({'predictor': x_train.columns,'coef': ridge.coef_.flatten()})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "ax.stem(df_coeficientes.predictor, df_coeficientes.coef, markerfmt=' ')\n",
    "plt.xticks(rotation=45, ha='right', size=10)\n",
    "ax.set_xlabel('Variable')\n",
    "ax.set_ylabel('Coeficiente')\n",
    "ax.set_title('Coeficientes de los predictores del modelo')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = ridge.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_test - y_test_pred))\n",
    "\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(\n",
    "    alphas=np.logspace(-5, 1, 500),\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "lasso.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = lasso.alphas_\n",
    "coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    modelo_aux = Lasso(alpha=alpha)\n",
    "    modelo_aux.fit(x_train, y_train)\n",
    "    coefs.append(modelo_aux.coef_.flatten())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "ax.plot(alphas, coefs, label=x_train.columns)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('coeficientes')\n",
    "ax.legend()\n",
    "ax.set_title('Coeficientes del modelo en función de la regularización')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de features incluidas (parámetros !=0) en función de alpha\n",
    "alphas = lasso.alphas_\n",
    "n_predictores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    modelo_aux = Lasso(alpha=alpha)\n",
    "    modelo_aux.fit(x_train, y_train)\n",
    "    coef_no_cero = np.sum(modelo_aux.coef_.flatten() != 0)\n",
    "    n_predictores.append(coef_no_cero)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "ax.plot(alphas, n_predictores)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('Cantidad de predictores')\n",
    "ax.set_title('Features incluidas en función de la regularización')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolución del error de validación cruzada en función de alpha\n",
    "\n",
    "# modelo.mse_path almacena el MSE de CV para cada valor de alpha.\n",
    "\n",
    "mse_cv = lasso.mse_path_.mean(axis=1)\n",
    "\n",
    "# Se aplica la raíz cuadrada para pasar de mse a rmse\n",
    "rmse_cv = np.sqrt(mse_cv)\n",
    "\n",
    "# Calcula R2\n",
    "r2_cv = 1 - (mse_cv / np.var(y_train))\n",
    "\n",
    "# Se identifica el mejor\n",
    "min_rmse     = np.min(rmse_cv)\n",
    "optimo       = lasso.alphas_[np.argmin(rmse_cv)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "ax.plot(lasso.alphas_, rmse_cv)\n",
    "ax.set_xscale('log')\n",
    "ax.set_title('Evolución del error en función de la regularización')\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('RMSE')\n",
    "\n",
    "ax.plot(optimo, min_rmse, marker='o', markersize=10, color=\"red\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métrica de train cross-validation\n",
    "print(f\"El valor óptimo de alpha es: {optimo:.2f} con un RMSE de {min_rmse:.2f} y un R² de {r2_cv[np.argmin(rmse_cv)]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficientes del modelo\n",
    "\n",
    "df_coeficientes = pd.DataFrame({'predictor': x_train.columns,'coef': lasso.coef_.flatten()})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "\n",
    "ax.stem(df_coeficientes.predictor, df_coeficientes.coef, markerfmt=' ')\n",
    "plt.xticks(rotation=45, ha='right', size=10)\n",
    "ax.set_xlabel('Variable')\n",
    "ax.set_ylabel('Coeficiente')\n",
    "ax.set_title('Coeficientes de los predictores del modelo')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lasso.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_test - y_test_pred))\n",
    "\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = ElasticNetCV(\n",
    "    l1_ratio        = [0.2, 0.4, 0.5, 0.6, 0.8],\n",
    "    alphas          = np.logspace(-3, 6, 200),\n",
    "    cv              = 10\n",
    ")\n",
    "\n",
    "en.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error medio de las 10 particiones por cada valor de alpha y l1_ratio\n",
    "mean_error_cv = en.mse_path_.mean(axis =2)\n",
    "\n",
    "# El resultado es un array de dimensiones (n_l1_ratio, n_alpha) se convierte en un dataframe\n",
    "df_resultados_cv = pd.DataFrame(\n",
    "                        data   = mean_error_cv.flatten(),\n",
    "                        index  = pd.MultiIndex.from_product(\n",
    "                                    iterables = [en.l1_ratio, en.alphas_],\n",
    "                                    names     = ['l1_ratio', 'en.alphas_']\n",
    "                                 ),\n",
    "                        columns = [\"mse_cv\"]\n",
    "                    )\n",
    "\n",
    "df_resultados_cv['rmse_cv'] = np.sqrt(df_resultados_cv['mse_cv'])\n",
    "df_resultados_cv = df_resultados_cv.reset_index().sort_values('mse_cv', ascending = True)\n",
    "df_resultados_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "df_resultados_cv.groupby('l1_ratio')['rmse_cv'].min().plot(ax = ax)\n",
    "ax.set_title('Evolución del error CV en función de la l1_ratio')\n",
    "ax.set_xlabel('l1_ratio')\n",
    "ax.set_ylabel('rmse_cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejor valor alpha y l1_ratio_ encontrado\n",
    "\n",
    "print(f\"Mejor valor de alpha encontrado: {en.alpha_}\")\n",
    "print(f\"Mejor valor de l1_ratio encontrado: {en.l1_ratio_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficientes del modelo\n",
    "df_coeficientes = pd.DataFrame({'predictor': x_train.columns,'coef': en.coef_.flatten()})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "\n",
    "ax.stem(df_coeficientes.predictor, df_coeficientes.coef, markerfmt=' ')\n",
    "plt.xticks(rotation=45, ha='right', size=10)\n",
    "ax.set_xlabel('Variable')\n",
    "ax.set_ylabel('Coeficiente')\n",
    "ax.set_title('Coeficientes de los predictores del modelo')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = en.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_test - y_test_pred))\n",
    "\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
